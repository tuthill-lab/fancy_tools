{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "import connectome_create\n",
    "# viz_method = one of ['itkwidgets', 'vtk']\n",
    "viz_method = 'vtk'\n",
    "\n",
    "# import some of our favorite packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# import nglui.statebuilder as ngstbld\n",
    "\n",
    "# # this is the EM specific package for querying the EM data\n",
    "from caveclient import CAVEclient\n",
    "\n",
    "\n",
    "# from meshparty import trimesh_io, trimesh_vtk\n",
    "# from meshparty import skeletonize, skeleton_io, skeleton\n",
    "# import cloudvolume\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import json\n",
    "\n",
    "# with open(Path.home() / '.cloudvolume/secrets/'/'cave-secret.json') as f:\n",
    "#         tokens = json.load(f)\n",
    "        \n",
    "# seg_source = 'graphene://https://cave.fanc-fly.com/segmentation/table/mar2021_prod'\n",
    "# cv = cloudvolume.CloudVolume(cloudpath=seg_source, use_https=True, secrets=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datastack_name = 'fanc_production_mar2021'\n",
    "\n",
    "client = CAVEclient(datastack_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# client = CAVEclient()\n",
    "\n",
    "# # if not os.path.isfile(os.path.expanduser(\"~/.cloudvolume/secrets/cave-secret.json\")):\n",
    "# client.auth.get_new_token(open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have not yet setup this computer, uncomment this below line\n",
    "# paste the token from the website in, and run the line\n",
    "\n",
    "# client.auth.save_token(token=\"c14cd7a3e18a1a697716a399afbf5778\", overwrite=True)\n",
    "\n",
    "# then comment or delete the line as you don't need to run it on this computer  again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load premotor neuron table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre_to_mn_df = connectome_create.load_pre_to_mn_df(ext='matched_typed_with_nt')\n",
    "print(pre_to_mn_df.shape)\n",
    "pre_to_mn_df.loc['sensory'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a premotor connectome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First get all synapses by querying chunks of segids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the synapses, then cross tab at the end\n",
    "# Peel off the index of the pre_to_mn list, which has info on premotor type and segIDs\n",
    "idx_df = pre_to_mn_df.index.to_frame()\n",
    "idx_df.index=idx_df.segID\n",
    "print(idx_df.shape)\n",
    "idx_df.head()\n",
    "\n",
    "cnt = 0\n",
    "xi = 0\n",
    "delta = 16\n",
    "xf = delta\n",
    "\n",
    "pre_to_pre_syn_df = None\n",
    "\n",
    "timestamp = connectome_create.get_timestamp()\n",
    "\n",
    "# Iterate over the premotor neurons in chunks\n",
    "pmn_sids = idx_df.segID.to_list()\n",
    "\n",
    "while xf<=idx_df.shape[0]:\n",
    "    sids = pmn_sids[xi:xf]\n",
    "    pmn_syn_df = client.materialize.synapse_query(pre_ids = sids,timestamp=timestamp) # Takes list\n",
    "    \n",
    "    print(pmn_syn_df.shape)\n",
    "\n",
    "    if pre_to_pre_syn_df is None:\n",
    "        pre_to_pre_syn_df = pmn_syn_df\n",
    "    else:\n",
    "        pre_to_pre_syn_df = pd.concat([pre_to_pre_syn_df,pmn_syn_df],axis=0,join='outer')\n",
    "    \n",
    "    cnt=cnt+1\n",
    "    if cnt % 4 == 0:\n",
    "        print('{} of {} rows complete, idx = {}'.format(xf,idx_df.shape[0],sids[-1]))\n",
    "        print(pre_to_pre_syn_df.shape)\n",
    "    xi=xf\n",
    "    if xf+delta < idx_df.shape[0]:\n",
    "        xf=xf+delta\n",
    "    elif xf==idx_df.shape[0]:\n",
    "        break\n",
    "    else:\n",
    "        xf = idx_df.shape[0]\n",
    "\n",
    "# This takes more like 4 minutes.\n",
    "pre_to_pre_syn_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then perform a crosstabulation of sorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, this is a crazy piece of code to calculate the crosstabulation\n",
    "# Currently, the dataframe is too large to perform crosstab. supposedly, pandas 1.4 solves this issue, but I'm having a hard time updating\n",
    "\n",
    "# first, for readability, just take the columns of interest\n",
    "df = pre_to_pre_syn_df.loc[:,['pre_pt_root_id','post_pt_root_id']]\n",
    "\n",
    "# Then use factorize, which encodes each occurance in a list, in this case of paired root_ids, pre and post\n",
    "ij,tups = pd.factorize(list(zip(*map(df.get,df))))\n",
    "\n",
    "# Then create a dictionary out of each occurance, storing the number of factorized codes\n",
    "result = dict(zip(tups, np.bincount(ij)))\n",
    "\n",
    "# Finally, turn the result into series, with the tuples of pt_root_ids as the multiindex\n",
    "tupseries = pd.Series(result)\n",
    "\n",
    "# Finally, threshold by the connection strength and perfom the unstack operation\n",
    "thresh = 3 # Note, tried this with 5, but then some of the premotors make no connections\n",
    "thresholdedtupseries = tupseries[tupseries>=thresh]\n",
    "prelim_pre_to_pre_df = thresholdedtupseries.unstack(fill_value=0)\n",
    "\n",
    "prelim_pre_to_pre_df = prelim_pre_to_pre_df.T\n",
    "prelim_pre_to_pre_df.shape\n",
    "\n",
    "# takes ~ 15 seconds,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prelim_pre_to_pre_df.shape)\n",
    "pre_to_pre_df = prelim_pre_to_pre_df\n",
    "print(pre_to_pre_df.shape)\n",
    "\n",
    "leftout = idx_df[~idx_df.segID.isin(prelim_pre_to_pre_df.columns)]\n",
    "leftout # nothing should be left out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectome_create.save_pre_to_df(pre_to_pre_df,ext='_no_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('fanc_analysis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b315cc870c8bed6572d217175f80b2213d2d88f8d1534b43b9efee2de96efaec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
